"""ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ ì ì¬ - Markdown íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ pgvector ì €ì¥."""

import argparse
import re
from dataclasses import dataclass, field
from pathlib import Path

from src.embedding import embed
from src.vectorstore import VectorStore

# í—¤ë”(##) ìˆ˜ì¤€ì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ë§¤í•‘
CATEGORY_KEYWORDS: dict[str, list[str]] = {
    "naming": ["ë„¤ì´ë°", "ì´ë¦„", "ëª…ëª…", "naming", "ë³€ìˆ˜ëª…", "í•¨ìˆ˜ëª…", "í´ë˜ìŠ¤ëª…", "ë©”ì„œë“œëª…", "íŒ¨í‚¤ì§€ëª…"],
    "error_handling": ["ì—ëŸ¬", "ì˜ˆì™¸", "exception", "error", "ë¦¬ì†ŒìŠ¤ ì •ë¦¬", "null"],
    "security": ["ë³´ì•ˆ", "security", "sql ì¸ì ì…˜", "ë¯¼ê° ì •ë³´", "ì…ë ¥ê°’ ê²€ì¦", "injection"],
    "performance": ["ì„±ëŠ¥", "performance", "n+1", "ë¹„ë™ê¸°", "ìºì‹œ", "í˜ì´ì§€ë„¤ì´ì…˜", "ì»¬ë ‰ì…˜"],
    "code_structure": ["ì½”ë“œ êµ¬ì¡°", "import", "í•¨ìˆ˜ í¬ê¸°", "ë©”ì„œë“œ í¬ê¸°", "íƒ€ì… íŒíŠ¸", "ë¡œê¹…", "êµ¬ì¡°"],
}

BATCH_SIZE = 32


@dataclass
class Chunk:
    content: str
    category: str | None = None
    source: str = ""
    chunk_index: int = 0
    headers: list[str] = field(default_factory=list)


def detect_category(headers: list[str], content: str) -> str | None:
    """í—¤ë”ì™€ ë³¸ë¬¸ ë‚´ìš©ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ë¡ í•œë‹¤."""
    text = " ".join(headers).lower() + " " + content[:200].lower()
    for category, keywords in CATEGORY_KEYWORDS.items():
        if any(kw in text for kw in keywords):
            return category
    return None


def chunk_markdown(text: str, source: str) -> list[Chunk]:
    """Markdown ë¬¸ì„œë¥¼ ## í—¤ë” ë‹¨ìœ„ë¡œ ì²­í‚¹í•œë‹¤.

    ì „ëµ:
    - ## (h2) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì£¼ì œë³„ ë…ë¦½ ì²­í¬ ìƒì„±
    - ìƒìœ„ í—¤ë”(#, h1)ëŠ” ëª¨ë“  í•˜ìœ„ ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
    - ### (h3) ì´í•˜ì˜ ì†Œì œëª©ì€ í•´ë‹¹ ## ì„¹ì…˜ì— í¬í•¨
    """
    lines = text.split("\n")
    chunks: list[Chunk] = []

    h1_header = ""
    current_headers: list[str] = []
    current_lines: list[str] = []
    chunk_index = 0

    def flush():
        nonlocal chunk_index
        if not current_lines:
            return
        body = "\n".join(current_lines).strip()
        if not body:
            return

        # ìƒìœ„ í—¤ë”ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
        header_context = "\n".join(current_headers)
        full_content = f"{header_context}\n\n{body}" if header_context else body

        chunks.append(Chunk(
            content=full_content,
            category=detect_category(current_headers, body),
            source=source,
            chunk_index=chunk_index,
            headers=list(current_headers),
        ))
        chunk_index += 1

    in_code_block = False
    for line in lines:
        # ì½”ë“œ ë¸”ë¡(```) í† ê¸€ â€” ì½”ë“œ ë¸”ë¡ ì•ˆì˜ #ì€ í—¤ë”ê°€ ì•„ë‹˜
        if line.startswith("```"):
            in_code_block = not in_code_block
            current_lines.append(line)
            continue

        if in_code_block:
            current_lines.append(line)
            continue

        # h1 í—¤ë”: ë¬¸ì„œ ì œëª© â€” ëª¨ë“  ì²­í¬ì— í¬í•¨
        if re.match(r"^# [^#]", line):
            h1_header = line
            continue

        # h2 í—¤ë”: ì²­í‚¹ ê²½ê³„
        if re.match(r"^## [^#]", line):
            flush()
            current_lines = []
            current_headers = [h1_header, line] if h1_header else [line]
            continue

        # h3 ì´í•˜: ê°™ì€ ì²­í¬ì— í¬í•¨
        current_lines.append(line)

    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
    flush()

    return chunks


def ingest_file(path: Path, store: VectorStore) -> int:
    """ë‹¨ì¼ Markdown íŒŒì¼ì„ ì²­í‚¹í•˜ì—¬ ë²¡í„° DBì— ì ì¬í•œë‹¤."""
    text = path.read_text(encoding="utf-8")
    chunks = chunk_markdown(text, source=str(path))

    if not chunks:
        print(f"  âš  {path.name}: ì²­í¬ ì—†ìŒ (ê±´ë„ˆëœ€)")
        return 0

    # ë°°ì¹˜ ì„ë² ë”©
    stored = 0
    for i in range(0, len(chunks), BATCH_SIZE):
        batch = chunks[i : i + BATCH_SIZE]
        texts = [c.content for c in batch]
        embeddings = embed(texts)

        items = [
            {
                "content": chunk.content,
                "embedding": emb,
                "category": chunk.category,
                "source": chunk.source,
                "chunk_index": chunk.chunk_index,
            }
            for chunk, emb in zip(batch, embeddings)
        ]
        store.insert_batch(items)
        stored += len(items)

    return stored


def ingest_directory(source_dir: str) -> int:
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Markdown íŒŒì¼ì„ ì ì¬í•œë‹¤."""
    source_path = Path(source_dir)
    if not source_path.is_dir():
        raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_dir}")

    md_files = sorted(source_path.glob("**/*.md"))
    if not md_files:
        print(f"âš  {source_dir}ì—ì„œ Markdown íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    store = VectorStore()
    total = 0

    print(f"ğŸ“‚ {source_dir}ì—ì„œ {len(md_files)}ê°œ íŒŒì¼ ë°œê²¬")
    for path in md_files:
        count = ingest_file(path, store)
        print(f"  âœ… {path.name}: {count}ê°œ ì²­í¬ ì ì¬")
        total += count

    print(f"\nì´ {total}ê°œ ì²­í¬ ì ì¬ ì™„ë£Œ")
    return total


def main():
    parser = argparse.ArgumentParser(description="ê°€ì´ë“œë¼ì¸ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì ì¬í•©ë‹ˆë‹¤.")
    parser.add_argument("--source", required=True, help="Markdown íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    args = parser.parse_args()

    ingest_directory(args.source)


if __name__ == "__main__":
    main()
